import streamlit as st
import pandas as pd
import io
import time
import unicodedata
import re
import gc

# è¡Œæ•°åˆ¶é™ï¼ˆ1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šï¼‰
MAX_ROWS_PER_FILE = 10000

# å¹´2æ¡â†’4æ¡å¤‰æ›
def convert_short_year_to_full(date_str: str) -> str:
    parts = date_str.split("/")
    if len(parts) == 3 and len(parts[0]) == 2:
        year = int(parts[0])
        year += 2000 if year < 70 else 1900
        return f"{year}/{parts[1]}/{parts[2]}"
    return date_str

# ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–ï¼ˆä¸¸æ•°å­—ãƒ»å…¨è§’ãƒ»ç©ºç™½ãªã©å‰Šé™¤ï¼‰
def normalize_column_name(col_name: str) -> str:
    col_name = unicodedata.normalize('NFKC', col_name)
    col_name = re.sub(r'[\sã€€\t\r\nâ‘ -â‘³ã‰‘-ã‰Ÿâ‘´-â’‡â“ª-â“¿â‘ -â“¾â‘ -â‘©]', '', col_name)
    return col_name.lower()

# CSVèª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®šä»˜ãï¼‰
def load_csv(uploaded_file: io.BytesIO) -> pd.DataFrame:
    encodings_to_try = ['utf-8', 'cp932', 'shift_jis', 'utf-16', 'utf-8-sig', 'latin1']
    df = None
    for encoding in encodings_to_try:
        uploaded_file.seek(0)
        try:
            df = pd.read_csv(uploaded_file, dtype=str, encoding=encoding, engine='python')
            break
        except Exception:
            continue

    if df is None:
        st.warning(f"{uploaded_file.name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆå¯¾å¿œã§ãã‚‹æ–‡å­—ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰")
        return pd.DataFrame()

    # ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–ã¨ãƒãƒƒãƒ”ãƒ³ã‚°
    normalized_columns = {col: normalize_column_name(col) for col in df.columns}
    rename_map = {
        col: 'æ—¥ä»˜' if 'æ—¥ä»˜' in norm or 'date' in norm or 'day' in norm
        else 'æ™‚åˆ»' if 'æ™‚åˆ»' in norm or 'time' == norm
        else col
        for col, norm in normalized_columns.items()
    }
    df.rename(columns=rename_map, inplace=True)

    if 'æ—¥ä»˜' not in df.columns or 'æ™‚åˆ»' not in df.columns:
        st.warning(f"{uploaded_file.name} ã« 'æ—¥ä»˜' ã¾ãŸã¯ 'æ™‚åˆ»' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    # 2é‡ãƒ˜ãƒƒãƒ€ãƒ¼å¯¾å¿œ
    df = df[~df['æ—¥ä»˜'].str.contains('æ—¥ä»˜', na=False) & ~df['æ™‚åˆ»'].str.contains('æ™‚åˆ»', na=False)]

    df['æ—¥ä»˜'] = df['æ—¥ä»˜'].astype(str).str.replace(r'[\sã€€\t\r\n]+', '', regex=True)
    df['æ™‚åˆ»'] = df['æ™‚åˆ»'].astype(str).str.replace(r'[\sã€€\t\r\n]+', '', regex=True)
    df = df[(df['æ—¥ä»˜'] != '') & (df['æ™‚åˆ»'] != '')]
    if df.empty:
        st.warning(f"{uploaded_file.name} ã«æœ‰åŠ¹ãªè¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    df['æ—¥ä»˜'] = df['æ—¥ä»˜'].apply(convert_short_year_to_full)

    try:
        df['æ—¥ä»˜'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Y/%m/%d').dt.strftime('%Y-%m-%d')
    except Exception as e:
        st.warning(f"{uploaded_file.name} ã®æ—¥ä»˜å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return pd.DataFrame()

    try:
        df['æ™‚åˆ»'] = pd.to_datetime(df['æ™‚åˆ»'], errors='coerce').dt.strftime('%H:%M:%S')
    except Exception as e:
        st.warning(f"{uploaded_file.name} ã®æ™‚åˆ»å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return pd.DataFrame()

    dt_str = df['æ—¥ä»˜'] + ' ' + df['æ™‚åˆ»']
    df['datetime'] = pd.to_datetime(dt_str, errors='coerce')
    df.dropna(subset=['datetime'], inplace=True)

    # è¡Œæ•°åˆ¶é™
    if len(df) > MAX_ROWS_PER_FILE:
        st.info(f"{uploaded_file.name} ã¯ {MAX_ROWS_PER_FILE} è¡Œã¾ã§ã«åˆ¶é™ã•ã‚Œã¾ã™ã€‚")
        df = df.iloc[:MAX_ROWS_PER_FILE]

    return df.reset_index(drop=True)

# é–“å¼•ãå‡¦ç†
def filter_by_interval(df: pd.DataFrame, interval_seconds: int) -> pd.DataFrame:
    if 'datetime' not in df.columns:
        st.error("datetimeåˆ—ã‚‚ã—ãã¯æ—¥ä»˜ãƒ»æ™‚åˆ»åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
        st.stop()

    if interval_seconds <= 0:
        return df.reset_index(drop=True)

    df['datetime_rounded'] = df['datetime'].dt.floor(f'{interval_seconds}S')
    reduced = df.groupby('datetime_rounded').first().copy()
    reduced.rename(columns={'datetime_rounded': 'datetime'}, inplace=True)
    reduced.reset_index(inplace=True)

    important_cols = [col for col in ['å¾ªç’°æ°´æµé‡'] if col in reduced.columns]
    if important_cols:
        reduced = reduced.dropna(subset=important_cols)

    return reduced.reset_index(drop=True)

# CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ç”Ÿæˆ
def generate_csv(df: pd.DataFrame) -> bytes:
    buf = io.StringIO()
    df.to_csv(buf, index=False, encoding='utf-8-sig')
    return buf.getvalue().encode('utf-8-sig')

# æŠ½å‡ºé–“éš”ã‚»ãƒ¬ã‚¯ã‚¿
def select_interval() -> int:
    options = ['å…¨ä»¶æŠ½å‡º(0)', '5ç§’', '10ç§’', '30ç§’', '1åˆ†', '5åˆ†', '10åˆ†']
    seconds_map = {
        'å…¨ä»¶æŠ½å‡º(0)': 0,
        '5ç§’': 5,
        '10ç§’': 10,
        '30ç§’': 30,
        '1åˆ†': 60,
        '5åˆ†': 300,
        '10åˆ†': 600
    }
    option = st.selectbox('æŠ½å‡ºé–“éš”ã‚’é¸æŠ', options, index=4)
    return seconds_map[option]

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ããƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°
@st.cache_data(show_spinner=False)
def load_and_process_file(file: io.BytesIO) -> pd.DataFrame:
    return load_csv(file)

# ã‚¢ãƒ—ãƒªæœ¬ä½“
def main():
    st.set_page_config(page_title='ãƒ­ã‚°æ•´å½¢ á”¦--á”¨', layout='centered', initial_sidebar_state='expanded')
    st.title('ãƒ­ã‚°æ•´å½¢ á”¦ê™¬á”¨')

    interval_seconds = select_interval()
    uploaded_files = st.file_uploader('CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„', type=['csv'], accept_multiple_files=True)
    start_clicked = st.button('â–¶ ã‚¹ã‚¿ãƒ¼ãƒˆ')

    if start_clicked:
        if not uploaded_files:
            st.warning('CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„')
            st.stop()

        progress = st.progress(0)
        status_text = st.empty()
        total_files = len(uploaded_files)

        merged_df = pd.DataFrame()

        start_time = time.time()
        for idx, file in enumerate(uploaded_files):
            # èª­ã¿è¾¼ã¿ï¼‹é–“å¼•ã
            df = load_and_process_file(file)
            reduced_df = filter_by_interval(df, interval_seconds)

            # ãƒãƒ¼ã‚¸
            if not reduced_df.empty:
                merged_df = pd.concat([merged_df, reduced_df], ignore_index=True)

            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del df, reduced_df
            gc.collect()

            # é€²æ—è¡¨ç¤ºï¼ˆè»½é‡åŒ–ï¼‰
            if (idx + 1) % 3 == 0 or idx == total_files - 1:
                progress_percent = int((idx + 1) / total_files * 90)
                progress.progress(progress_percent)

            status_text.text(f"ğŸ“„ å‡¦ç†ä¸­: {idx + 1}/{total_files} - {file.name}")

        total_time = time.time() - start_time

        if merged_df.empty:
            st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.stop()

        # ä¸¦ã³æ›¿ãˆï¼ˆæœ€å¾Œã ã‘ï¼‰
        merged_df = merged_df.drop_duplicates().sort_values('datetime').reset_index(drop=True)

        progress.progress(100)
        status_text.text("âœ… å®Œäº†ï¼")

        st.success(
            f"å‡¦ç†å®Œäº†ï¼ åˆè¨ˆ {len(merged_df)} ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸ\n"
            f"å‡¦ç†æ™‚é–“: {total_time:.2f} ç§’"
        )

        csv_bytes = generate_csv(merged_df)
        st.download_button('ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰', csv_bytes, file_name='filtered_interval_data.csv', mime='text/csv')


if __name__ == '__main__':
    main()
